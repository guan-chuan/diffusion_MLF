# enhanced_diffusion_model_fixed.py ä»£ç å®¡æŸ¥æŠ¥å‘Š

## ğŸ“‹ æ¦‚è¿°
æœ¬æ–‡ä»¶å®ç°äº†ä¸€ä¸ªæ‰©æ•£æ¨¡å‹ï¼Œç”¨äºå¤šå±‚è–„è†œç»“æ„çš„é€†å‘è®¾è®¡ã€‚ä½¿ç”¨äº†ç‰©ç†ç¥ç»ç½‘ç»œï¼ˆPNNï¼‰ä½œä¸ºä»£ç†æ¥è®¡ç®—å…‰å­¦ç‰¹æ€§ã€‚

---

## âœ… æ­£ç¡®çš„æ¶æ„å…ƒç´ 

### 1. å™ªå£°è°ƒåº¦ç¨‹åº (NoiseScheduler) - æ­£ç¡® âœ…
- **ç¬¬36-64è¡Œ**: æ­£ç¡®çš„å‰å‘æ‰©æ•£è¿‡ç¨‹
- âœ… ä½¿ç”¨çº¿æ€§ beta è°ƒåº¦
- âœ… æ­£ç¡®è®¡ç®— `alphas_cumprod`
- âœ… `q_sample` ä½¿ç”¨æ­£ç¡®çš„é«˜æ–¯æ··åˆå…¬å¼: `sqrt(alpha_bar)*x0 + sqrt(1-alpha_bar)*eps`
- âœ… åéªŒæ–¹å·®è®¡ç®—æ­£ç¡®

### 2. æ—¶é—´åµŒå…¥ (sinusoidal_time_embedding) - æ­£ç¡® âœ…
- **ç¬¬142-151è¡Œ**: æ ‡å‡†çš„æ­£å¼¦æ—¶é—´ç¼–ç 
- âœ… ä½¿ç”¨ log(10000) å’Œ exp çš„æ ‡å‡†å½¢å¼
- âœ… æ­£ç¡®ç»“åˆ sin/cos

---

## ğŸ”´ å‘ç°çš„é—®é¢˜

### é—®é¢˜ 1ï¸âƒ£: ResBlock ä¸­çš„æ¡ä»¶åç½®å¤„ç†ä¸å½“ (CRITICAL)
**ä½ç½®**: ç¬¬163-172è¡Œ

```python
def forward(self, x, t_emb, cond_emb=None):
    h = self.norm1(x)
    h = self.act(self.fc1(h))
    bias = self.time_proj(t_emb).unsqueeze(1) if t_emb is not None else 0.0
    if cond_emb is not None:
        bias = bias + self.cond_proj(cond_emb).unsqueeze(1)  # âš ï¸ å¼ é‡ + å¼ é‡ æˆ– æµ®ç‚¹æ•° + å¼ é‡
    h = h + bias
```

**é—®é¢˜**:
- å½“ `t_emb is None` æ—¶ï¼Œ`bias = 0.0` (Python float)
- å½“ `cond_emb is not None` æ—¶ï¼Œå°è¯•å¯¹ float å’Œå¼ é‡è¿›è¡ŒåŠ æ³•
- å¼ é‡å¹¿æ’­ä¼šå¤±è´¥æˆ–äº§ç”Ÿä¸ç¬¦åˆé¢„æœŸçš„è¡Œä¸º

**ä¿®å¤å»ºè®®**:
```python
bias = self.time_proj(t_emb) if t_emb is not None else torch.zeros(x.shape[0], 1, self.hidden_dim, device=x.device)
```

---

### é—®é¢˜ 2ï¸âƒ£: åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼ (CFG) å®ç°ä¸ä¸€è‡´ (HIGH PRIORITY)
**ä½ç½®**: è®­ç»ƒ (ç¬¬424-431è¡Œ) vs é‡‡æ · (ç¬¬572-575è¡Œ)

**è®­ç»ƒæ—¶**:
```python
drop_mask = (torch.rand(B, device=self.device) < self.p_uncond)
pred_mat_noise, pred_thk_noise = self.model(x_t_mat, x_t_thk, timesteps, cond_emb, layer_mask, drop_mask)
```
- ä½¿ç”¨ `drop_mask` æ§åˆ¶æ¡ä»¶éšè—

**é‡‡æ ·æ—¶**:
```python
eps_cond_mat, eps_cond_thk = self.model(x, x_thk, t_tensor, cond_emb, layer_mask, drop_mask=None)
eps_uncond_mat, eps_uncond_thk = self.model(x, x_thk, t_tensor, cond_emb*0.0, layer_mask, drop_mask=None)
```
- é€šè¿‡å°† `cond_emb * 0` ä¼ é€’æ¥å®ç°æ— æ¡ä»¶
- ä½†æ¨¡å‹æ²¡æœ‰åœ¨è¿™é‡Œåº”ç”¨ `drop_mask`ï¼

**é—®é¢˜**: 
- è®­ç»ƒæ—¶æ¡ä»¶è¢«æ¨¡å‹ä¸¢å¼ƒï¼ˆé€šè¿‡ `drop_mask`ï¼‰
- é‡‡æ ·æ—¶æ¡ä»¶è¢«ç½®é›¶ä½†æ¨¡å‹ä»åœ¨å¤„ç†ï¼ˆå¯èƒ½ä»æœ‰æ¡ä»¶ä¿¡æ¯é€šè¿‡å…¶ä»–è·¯å¾„ï¼‰
- è¿™ä¼šå¯¼è‡´è®­ç»ƒ-æ¨ç†ä¸åŒ¹é…

**ä¿®å¤å»ºè®®**:
```python
# é‡‡æ ·æ—¶ä¹Ÿåº”è¯¥ä½¿ç”¨ drop_mask
drop_mask_cond = torch.zeros(B, dtype=torch.bool, device=device)  # ä¿ç•™æ¡ä»¶
eps_cond_mat, eps_cond_thk = self.model(x, x_thk, t_tensor, cond_emb, layer_mask, drop_mask=drop_mask_cond)

drop_mask_uncond = torch.ones(B, dtype=torch.bool, device=device)  # ä¸¢å¼ƒæ¡ä»¶
eps_uncond_mat, eps_uncond_thk = self.model(x, x_thk, t_tensor, cond_emb, layer_mask, drop_mask=drop_mask_uncond)
```

---

### é—®é¢˜ 3ï¸âƒ£: PNN ä»£ç†ä¸­çš„ç´¢å¼•é€‰æ‹©æ“ä½œé”™è¯¯ (HIGH)
**ä½ç½®**: ç¬¬317-319è¡Œ

```python
if self.reorder_idx is not None:
    idx = self.reorder_idx.to(materials_probs.device)
    materials_probs = torch.index_select(materials_probs, dim=-1, index=idx)
```

**é—®é¢˜**:
- `materials_probs` çš„å½¢çŠ¶æ˜¯ `(B, L, V)`ï¼ˆææ–™æ¦‚ç‡ï¼‰
- `reorder_idx` çš„å½¢çŠ¶æ˜¯ `(V_pnn,)`
- ä½¿ç”¨ `dim=-1` å’Œ 1D ç´¢å¼•å¼ é‡åº”è¯¥å¯ä»¥å·¥ä½œ...ä½†è¦æ£€æŸ¥ `reorder_idx` çš„é•¿åº¦æ˜¯å¦æ­£ç¡®

**æ½œåœ¨é£é™©**:
- å¦‚æœ `reorder_idx` é•¿åº¦ä¸ç­‰äº `materials_probs` çš„æœ€åä¸€ç»´ï¼Œä¼šæŠ¥é”™
- ä»£ç æ²¡æœ‰æ£€æŸ¥è¯æ±‡è¡¨å¤§å°æ˜¯å¦ä¸€è‡´

**ä¿®å¤å»ºè®®**: æ·»åŠ éªŒè¯
```python
if self.reorder_idx is not None:
    assert len(self.reorder_idx) == materials_probs.shape[-1], \
        f"Vocab size mismatch: reorder_idx={len(self.reorder_idx)}, materials_probs={materials_probs.shape[-1]}"
    idx = self.reorder_idx.to(materials_probs.device)
    materials_probs = torch.index_select(materials_probs, dim=-1, index=idx)
```

---

### é—®é¢˜ 4ï¸âƒ£: x0 é‡æ„ä¸­ç¼ºå°‘æ¸©åº¦å‚æ•° (MEDIUM)
**ä½ç½®**: ç¬¬388-398è¡Œ

```python
def _reconstruct_x0(self, x_t_mat, x_t_thk, pred_mat_noise, pred_thk_noise, timesteps):
    alpha_bar_t = self.scheduler.alphas_cumprod[timesteps].to(device)
    sqrt_alpha_bar = torch.sqrt(alpha_bar_t).view(*shape)
    sqrt_1m = torch.sqrt(1.0 - alpha_bar_t).view(*shape)
    x0_mat = (x_t_mat - sqrt_1m * pred_mat_noise) / (sqrt_alpha_bar + 1e-12)
```

**é—®é¢˜**:
- è¿™ä½¿ç”¨çš„æ˜¯ x0 å…¬å¼ï¼ˆé¢„æµ‹ x0 è€Œä¸æ˜¯å™ªå£°ï¼‰
- å…¬å¼ä¼¼ä¹æ˜¯æ­£ç¡®çš„ï¼Œä½†æ¨¡å‹å®é™…ä¸Šé¢„æµ‹çš„æ˜¯å™ªå£° (epsilon) - éœ€è¦æ£€æŸ¥æ¨¡å‹çš„è®¾è®¡æ„å›¾
- å¦‚æœæ¨¡å‹è®¾è®¡ä¸ºé¢„æµ‹å™ªå£°ï¼Œåº”è¯¥ç›´æ¥è¿”å›ï¼Œè€Œä¸æ˜¯è®¡ç®— x0 çš„ä¸­é—´è¡¨ç¤º

**éªŒè¯ç‚¹**:
- æ¨¡å‹è¾“å‡ºå‘½åä¸º `mat_noise` å’Œ `thk_noise`ï¼ˆç¬¬220-221è¡Œï¼‰ï¼Œè¡¨ç¤ºé¢„æµ‹å™ªå£°
- ä½†åœ¨è®­ç»ƒæŸå¤±ä¸­ï¼ˆç¬¬434è¡Œï¼‰: `loss_noise = F.mse_loss(pred_mat_noise, eps_mat)`
- è¿™æ˜¯æ­£ç¡®çš„ - æ¨¡å‹é¢„æµ‹å™ªå£°ï¼ŒæŸå¤±å¯¹æ¯”å™ªå£°

**ç»“è®º**: è™½ç„¶åœ¨é‡æ„ä¸­è®¡ç®— x0ï¼Œä½†è¿™ç”¨äºç”Ÿæˆç‰©ç†ç»“æ„çš„æ¦‚ç‡ã€‚è¿™åœ¨é€»è¾‘ä¸Šæ˜¯åˆç†çš„ã€‚âœ…

---

### é—®é¢˜ 5ï¸âƒ£: EMA æ›´æ–°ä¸­çš„æ¢¯åº¦æµé—®é¢˜ (MEDIUM)
**ä½ç½®**: ç¬¬367-373è¡Œ

```python
def _update_ema(self):
    for n,p in self.model.named_parameters():
        if p.requires_grad:
            self.ema[n] = self.ema[n].to(p.device)
            self.ema[n].mul_(self.ema_decay).add_(p.detach(), alpha=1.0 - self.ema_decay)
```

**é—®é¢˜**:
- ä½¿ç”¨ `p.detach()` æ˜¯æ­£ç¡®çš„ - EMA ä¸åº”è¯¥æœ‰æ¢¯åº¦
- ä½† `self.ema[n]` éœ€è¦ä¹Ÿæ˜¯ detached å¼ é‡
- å®é™…ä¸Šï¼Œç¬¬346è¡Œåˆå§‹åŒ–æ—¶å·²ç»ä½¿ç”¨äº† `.detach().clone()`

**æ£€æŸ¥**: åˆå§‹åŒ–æ˜¯æ­£ç¡®çš„ âœ…
```python
self.ema = {n: p.detach().clone() for n,p in self.model.named_parameters() if p.requires_grad}
```

---

### é—®é¢˜ 6ï¸âƒ£: NoAttentionTransformerBlock ä¸­çš„æ©ç å¤„ç† (MEDIUM)
**ä½ç½®**: ç¬¬182-191è¡Œ

```python
def forward(self, x, mask=None):
    if mask is not None:
        x = x * mask.unsqueeze(-1).float()
    # ... transformer block ...
    if mask is not None:
        h = h * mask.unsqueeze(-1).float()
    return x + h
```

**é—®é¢˜**:
- æ©ç è¢«åº”ç”¨ä¸¤æ¬¡ï¼ˆå¼€å§‹å’Œç»“æŸï¼‰ï¼Œå¯èƒ½ä¼šå¯¼è‡´æœ‰æ•ˆå…ƒç´ è¢«åŒé‡æ©ç 
- å½“ mask æ˜¯å¸ƒå°”å¼ é‡æ—¶ï¼Œè½¬æ¢ä¸º float åæ— æ•ˆä½ç½®å˜ä¸º 0.0ï¼Œæœ‰æ•ˆä½ç½®ä¸º 1.0ï¼ˆæ­£ç¡®ï¼‰
- ä½†åœ¨æ®‹å·®è¿æ¥ä¸­ï¼Œæœ‰æ•ˆå…ƒç´ è¢«ä¹˜ä»¥æ©ç ä¸¤æ¬¡

**é€»è¾‘é—®é¢˜**: 
- è¾“å…¥ x è¢«æ©ç å¤„ç†åè¿›å…¥æ®‹å·®å—
- è¾“å‡º h åˆè¢«æ©ç å¤„ç†
- æœ€å `x + h` - è¿™æ—¶ x å·²ç»è¢«æ©ç äº†ï¼Œh ä¹Ÿè¢«æ©ç äº†
- è¿™å®é™…ä¸Šæ˜¯å¹³æ–¹æ©ç æ•ˆåº”ï¼ˆmasked * masked = problematicï¼‰

**ä¿®å¤å»ºè®®**:
```python
def forward(self, x, mask=None):
    if mask is not None:
        x = x * mask.unsqueeze(-1).float()
    h = self.norm1(x)
    h = self.act(self.fc1(h))
    h = self.fc2(h)
    h = self.norm2(h)
    # åªåœ¨è¾“å‡ºåº”ç”¨æ©ç ï¼Œä¸è¦åœ¨ä¸¤å¤„åº”ç”¨
    if mask is not None:
        h = h * mask.unsqueeze(-1).float()
    return x + h
```

---

### é—®é¢˜ 7ï¸âƒ£: æ¡ä»¶åµŒå…¥ç¼–ç å™¨åˆå§‹åŒ–ä½ç½® (HIGH)
**ä½ç½®**: ç¬¬354-365è¡Œ

```python
def _encode_spectrum(self, spectra):
    B = spectra.shape[0]
    if not hasattr(self, '_spec_encoder'):
        self._spec_encoder = nn.Sequential(...)  # âš ï¸ åŠ¨æ€åˆ›å»ºï¼
    return self._spec_encoder(spectra.to(self.device))
```

**ä¸¥é‡é—®é¢˜**:
- âŒ æ¡ä»¶ç¼–ç å™¨åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ‰è¢«åˆ›å»º
- âŒ å®ƒä¸æ˜¯æ¨¡å‹çš„æ­£å¼å‚æ•°ï¼ˆä¸é€šè¿‡ optimizerï¼‰
- âŒ è¿™ä¼šå¯¼è‡´å®ƒçš„æ¢¯åº¦æ— æ³•è¢«åå‘ä¼ æ’­ï¼

**è¯æ®**:
- ç¬¬345è¡Œ: `self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)`
- ä¼˜åŒ–å™¨åªä¼˜åŒ– `self.model` çš„å‚æ•°
- ä½† `_spec_encoder` æ˜¯ `self` çš„å±æ€§ï¼Œä¸è¢«åŒ…å«

**ä¸¥é‡åæœ**:
- æ¡ä»¶ç¼–ç å™¨çš„æƒé‡ä¸ä¼šè¢«æ›´æ–°
- å…‰è°±æ¡ä»¶ç¼–ç å§‹ç»ˆæ˜¯éšæœºåˆå§‹åŒ–
- ç‰©ç†æŸå¤±æ— æ³•å­¦åˆ°æœ‰æ„ä¹‰çš„æ¡ä»¶ä¿¡æ¯

**ä¿®å¤**:
```python
class EnhancedDiffusionModel:
    def __init__(self, ...):
        # ... 
        self.spec_encoder = nn.Sequential(
            nn.LayerNorm(self.spectrum_dim * 2),  # T + R
            nn.Linear(self.spectrum_dim * 2, 256),
            nn.SiLU(),
            nn.Linear(256, 128)
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨åº”åŒ…æ‹¬ spec_encoder
        self.optimizer = torch.optim.AdamW(
            list(self.model.parameters()) + list(self.spec_encoder.parameters()),
            lr=1e-4
        )
    
    def _encode_spectrum(self, spectra):
        return self.spec_encoder(spectra.to(self.device))
```

---

### é—®é¢˜ 8ï¸âƒ£: é‡‡æ ·ä¸­ç¼ºå°‘æ¡ä»¶å¤„ç†çš„æ¨¡å‹è°ƒç”¨ (HIGH)
**ä½ç½®**: ç¬¬572-575è¡Œ

```python
eps_cond_mat, eps_cond_thk = self.model(x, x_thk, t_tensor, cond_emb, layer_mask, drop_mask=None)
eps_uncond_mat, eps_uncond_thk = self.model(x, x_thk, t_tensor, cond_emb*0.0, layer_mask, drop_mask=None)
```

**é—®é¢˜**:
- `drop_mask=None` åœ¨ä¸¤æ¬¡è°ƒç”¨ä¸­éƒ½æ˜¯ None
- ä½†åœ¨è®­ç»ƒä¸­ï¼Œ`drop_mask` è¢«ç”¨æ¥é€‰æ‹©æ€§åœ°ä¸¢å¼ƒæ¡ä»¶ï¼ˆç¬¬425è¡Œï¼‰
- é‡‡æ ·ä¸­çš„æ¡ä»¶å¤„ç†æ–¹å¼ï¼ˆç½®é›¶ï¼‰ä¸è®­ç»ƒä¸­çš„æ–¹å¼ï¼ˆä½¿ç”¨ drop_maskï¼‰ä¸ä¸€è‡´

**å†æ£€æŸ¥æ¨¡å‹ä»£ç ** (ç¬¬210è¡Œ):
```python
def forward(self, mat_noisy, thk_noisy, timesteps, cond_emb, layer_mask, drop_mask=None):
```
- æ¨¡å‹æ¥å— `drop_mask`ï¼Œä½†æ²¡æœ‰åœ¨æ¨¡å‹ä¸­çœ‹åˆ°å®é™…ä½¿ç”¨å®ƒçš„ä»£ç ï¼

**å¤§é—®é¢˜**: `drop_mask` åœ¨æ¨¡å‹ä¸­æ²¡æœ‰è¢«ä½¿ç”¨ï¼
- å®ƒä½œä¸ºå‚æ•°ä¼ å…¥ä½†å®Œå…¨è¢«å¿½ç•¥äº†
- è¿™æ„å‘³ç€åˆ†ç±»å™¨è‡ªç”±å¼•å¯¼æ ¹æœ¬æ²¡æœ‰è¢«æ­£ç¡®å®ç°

---

## ğŸ¯ æ±‡æ€»è¡¨

| é—®é¢˜ | ä½ç½® | ä¸¥é‡çº§åˆ« | å½±å“ |
|------|------|--------|------|
| ResBlock åç½®å¤„ç† | 166è¡Œ | ğŸ”´ CRITICAL | å´©æºƒæˆ–é”™è¯¯è®¡ç®— |
| CFG å®ç°ä¸ä¸€è‡´ | 424-575è¡Œ | ğŸ”´ CRITICAL | è®­ç»ƒ-æ¨ç†ä¸åŒ¹é… |
| drop_mask æœªä½¿ç”¨ | æ¨¡å‹æœªå®ç° | ğŸ”´ CRITICAL | CFG æ— æ³•å·¥ä½œ |
| spec_encoder å‚æ•°æœªä¼˜åŒ– | 354-365è¡Œ | ğŸ”´ CRITICAL | æ¡ä»¶ä¿¡æ¯æ— æ³•å­¦ä¹  |
| PNN ç´¢å¼•å¯èƒ½é”™è¯¯ | 317-319è¡Œ | ğŸŸ¡ HIGH | è¯æ±‡è¡¨ä¸åŒ¹é…æ—¶å´©æºƒ |
| Transformer æ©ç åº”ç”¨åŒé‡ | 182-191è¡Œ | ğŸŸ¡ MEDIUM | æ•°å€¼ä¸ç¨³å®š |
| x0 é‡æ„é€»è¾‘ | 388è¡Œ | âœ… OK | - |

---

## ğŸ”§ æ¨èä¿®å¤ä¼˜å…ˆçº§

1. **ç¬¬ä¸€ä¼˜å…ˆ** (å¿…é¡»ä¿®å¤):
   - [ ] ä¿®å¤ spec_encoder åˆå§‹åŒ–å’Œä¼˜åŒ–
   - [ ] å®ç° drop_mask åœ¨æ¨¡å‹ä¸­çš„ä½¿ç”¨
   - [ ] ä¿®å¤ ResBlock åç½®å¤„ç†
   
2. **ç¬¬äºŒä¼˜å…ˆ** (åº”è¯¥ä¿®å¤):
   - [ ] ä¿®å¤ CFG é‡‡æ ·å®ç°
   - [ ] æ·»åŠ  PNN ç´¢å¼•éªŒè¯
   
3. **ç¬¬ä¸‰ä¼˜å…ˆ** (å¯é€‰):
   - [ ] ä¼˜åŒ– Transformer æ©ç åº”ç”¨

